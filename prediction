#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd


# In[ ]:


# Hyperparameters
input_size = 784  # Size of the latent vector for the Generator
g_hidden_size = 128
d_hidden_size = 128
output_size = 784 
batch_size = 64
lr = 0.0002
epochs = 4
# Dataset & DataLoader settings
length = 1024  #
real_data_size = output_size  # Same as Generator's output size


# In[ ]:


class Dataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = pd.read_excel('CAMELS.xlsx')

    def __getitem__(self, index):
        return self.data[index], torch.zeros(1)

    def __len__(self):
        return self.len
dataset = Dataset(real_data_size, length)
dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)


# In[ ]:


# Define the Generator
class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size, output_size),
            nn.Tanh()  # Assuming output is normalized between -1 and 1
        )

    def forward(self, x):
        return self.decoder(self.encoder (x))

    def get_feature(self, x):
        return self.encoder (x)
    
# Define the Discriminator
class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Discriminator, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.fc(x)


# Create the Generator and Discriminator
generator = Generator(input_size, g_hidden_size, output_size)
discriminator = Discriminator(output_size, d_hidden_size)

# Loss and Optimizers
criterion = nn.BCELoss()
optimizer_g = optim.Adam(generator.parameters(), lr=lr)
optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)

# Training Loop
for epoch in range(epochs):
    for i, data in enumerate(dataloader):  # Assuming you have a dataloader
        # Get real images and labels
        real_images, _ = data
        real_labels = torch.ones(batch_size, 1)
        
        # Train Discriminator with real samples
        optimizer_d.zero_grad()
        outputs = discriminator(real_images)
        d_loss_real = criterion(outputs, real_labels)
        d_loss_real.backward()

        # Generate fake images and train Discriminator
        noise = torch.randn(batch_size, input_size)
        fake_images = generator(noise)
        fake_labels = torch.zeros(batch_size, 1)
        outputs = discriminator(fake_images.detach())
        d_loss_fake = criterion(outputs, fake_labels)
        d_loss_fake.backward()
        optimizer_d.step()

        # Train Generator
        optimizer_g.zero_grad()
        outputs = discriminator(fake_images)
        g_loss = criterion(outputs, real_labels)
        g_loss.backward()
        optimizer_g.step()

    print(f'Epoch [{epoch+1}/{epochs}], d_loss: {d_loss_real+d_loss_fake:.4f}, g_loss: {g_loss:.4f}')

# Save the model or do further processing
generator_file = 'generator_model.pth'
discriminator_file = 'discriminator_model.pth'
torch.save(generator.state_dict(), generator_file)
torch.save(discriminator.state_dict(), discriminator_file)


# In[ ]:


class TLSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(TLSTMCell, self).__init__()
        self.hidden_size = hidden_size

        # Input gate
        self.W_xi = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.W_ci = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_i = nn.Parameter(torch.Tensor(hidden_size))

        # Forget gate
        self.W_xf = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.W_cf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_f = nn.Parameter(torch.Tensor(hidden_size))

        # Output gate
        self.W_xo = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.W_co = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_o = nn.Parameter(torch.Tensor(hidden_size))

        # Memory cell
        self.W_xc = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hc = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_c = nn.Parameter(torch.Tensor(hidden_size))

        self.init_weights()

    def init_weights(self):
        for p in self.parameters():
            if p.data.ndimension() >= 2:
                nn.init.xavier_uniform_(p.data)
            else:
                nn.init.zeros_(p.data)

    def forward(self, x, h_prev, c_prev):
        i_t = torch.sigmoid(self.W_xi @ x + self.W_hi @ h_prev + self.W_ci @ c_prev + self.b_i)
        f_t = torch.sigmoid(self.W_xf @ x + self.W_hf @ h_prev + self.W_cf @ c_prev + self.b_f)
        c_t = f_t * c_prev + i_t * torch.tanh(self.W_xc @ x + self.W_hc @ h_prev + self.b_c)
        o_t = torch.sigmoid(self.W_xo @ x + self.W_ho @ h_prev + self.W_co @ c_t + self.b_o)
        h_t = o_t * torch.tanh(c_t)

        return h_t, c_t 


# In[ ]:


generator = Generator(input_size, g_hidden_size, output_size)
generator.load_state_dict(torch.load('generator_model.pth'))
generator.eval()  # Set the generator to evaluation mode


# In[ ]:


feature_size = g_hidden_size  # Input feature size
hidden_size = 20  # Number of neurons in LSTM cell
num_epochs=100
num_iterations=100
sequence_length=8
# Create the TLSTM model
tlstm_cell = TLSTMCell(feature_size, hidden_size)
# Define the optimizer for TLSTM
optimizer = optim.Adam(tlstm_cell.parameters(), lr=0.001)


# In[ ]:


dataloader = DataLoader(dataset=dataset, batch_size=batch_size*sequence_length, shuffle=True)


# In[ ]:


for epoch in range(num_epochs):
    for i, data in enumerate(dataloader):  # Assuming you have a dataloader
        real_images, _ = data
        generated_data = generator.get_feature(real_images)

        # Reshape the generator output to be a sequence
        generated_data = generated_data.view(batch_size, sequence_length, feature_size)
        # Initialize hidden and cell states for TLSTM
        h_t, c_t = torch.zeros(batch_size, hidden_size), torch.zeros(batch_size, hidden_size)

        # Pass each time step of the generated data through TLSTM
        for t in range(sequence_length):
            h_t, c_t = tlstm_cell(generated_data[:, t, :], h_t, c_t)

        # Compute loss if applicable
        loss = loss_function(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f'Epoch {epoch+1}/{num_epochs} completed')

# Save the trained TLSTM model if needed
torch.save(tlstm_cell.state_dict(), 'tlstm_model.pth')


# In[ ]:




